{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Reference to: https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from options.train_options import TrainOptions\n",
    "from options.test_options import TestOptions\n",
    "from data import create_dataset\n",
    "from models import create_model\n",
    "from util.visualizer import Visualizer, save_images\n",
    "from util import html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Далее ниже в качестве базового GAN'а будет использоваться LSGAN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pix2pix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Options ---------------\n",
      "               batch_size: 1                             \n",
      "                    beta1: 0.5                           \n",
      "          checkpoints_dir: ./checkpoints                 \n",
      "           continue_train: False                         \n",
      "                crop_size: 256                           \n",
      "                 dataroot: ./facades/                    \n",
      "             dataset_mode: unaligned                     \n",
      "                direction: AtoB                          \n",
      "              display_env: main                          \n",
      "             display_freq: 400                           \n",
      "               display_id: 1                             \n",
      "            display_ncols: 4                             \n",
      "             display_port: 8097                          \n",
      "           display_server: http://localhost              \n",
      "          display_winsize: 256                           \n",
      "                    epoch: latest                        \n",
      "              epoch_count: 1                             \n",
      "                 gan_mode: lsgan                         \n",
      "                  gpu_ids: 0                             \n",
      "                init_gain: 0.02                          \n",
      "                init_type: normal                        \n",
      "                 input_nc: 3                             \n",
      "                  isTrain: True                          \t[default: None]\n",
      "                 lambda_A: 10.0                          \n",
      "                 lambda_B: 10.0                          \n",
      "          lambda_identity: 0.5                           \n",
      "                load_iter: 0                             \t[default: 0]\n",
      "                load_size: 286                           \n",
      "                       lr: 0.0002                        \n",
      "           lr_decay_iters: 50                            \n",
      "                lr_policy: linear                        \n",
      "         max_dataset_size: inf                           \n",
      "                    model: cycle_gan                     \n",
      "                 n_epochs: 100                           \n",
      "           n_epochs_decay: 100                           \n",
      "               n_layers_D: 3                             \n",
      "                     name: experiment_name               \n",
      "                      ndf: 64                            \n",
      "                     netD: basic                         \n",
      "                     netG: resnet_9blocks                \n",
      "                      ngf: 64                            \n",
      "               no_dropout: True                          \n",
      "                  no_flip: False                         \n",
      "                  no_html: False                         \n",
      "                     norm: instance                      \n",
      "              num_threads: 4                             \n",
      "                output_nc: 3                             \n",
      "                    phase: train                         \n",
      "                pool_size: 50                            \n",
      "               preprocess: resize_and_crop               \n",
      "               print_freq: 100                           \n",
      "             save_by_iter: False                         \n",
      "          save_epoch_freq: 5                             \n",
      "         save_latest_freq: 5000                          \n",
      "           serial_batches: False                         \n",
      "                   suffix:                               \n",
      "         update_html_freq: 1000                          \n",
      "                use_wandb: False                         \n",
      "                  verbose: False                         \n",
      "----------------- End -------------------\n"
     ]
    }
   ],
   "source": [
    "opt_pix2pix = TrainOptions().parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_pix2pix.model = \"pix2pix\"\n",
    "opt_pix2pix.dataset_mode = \"aligned\"\n",
    "opt_pix2pix.name = \"pix2pix: LSGAN\"\n",
    "opt_pix2pix.lambda_L1 = 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset [AlignedDataset] was created\n",
      "The number of training images = 400\n"
     ]
    }
   ],
   "source": [
    "dataset = create_dataset(opt_pix2pix)\n",
    "dataset_size = len(dataset)    # get the number of images in the dataset.\n",
    "print('The number of training images = %d' % dataset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize network with normal\n",
      "initialize network with normal\n",
      "model [Pix2PixModel] was created\n",
      "---------- Networks initialized -------------\n",
      "[Network G] Total number of parameters : 11.378 M\n",
      "[Network D] Total number of parameters : 2.768 M\n",
      "-----------------------------------------------\n",
      "create web directory ./checkpoints/pix2pix: LSGAN/web...\n"
     ]
    }
   ],
   "source": [
    "model = create_model(opt_pix2pix)      # create a model given opt.model and other options\n",
    "model.setup(opt_pix2pix)               # regular setup: load and print networks; create schedulers\n",
    "visualizer = Visualizer(opt_pix2pix)   # create a visualizer that display/save images and plots\n",
    "total_iters = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate 0.0002000 -> 0.0002000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agysar/miniconda3/envs/advanced_ml/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/home/agysar/miniconda3/envs/advanced_ml/lib/python3.8/site-packages/torchvision/transforms/transforms.py:287: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "/home/agysar/miniconda3/envs/advanced_ml/lib/python3.8/site-packages/torchvision/transforms/transforms.py:287: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "/home/agysar/miniconda3/envs/advanced_ml/lib/python3.8/site-packages/torchvision/transforms/transforms.py:287: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "/home/agysar/miniconda3/envs/advanced_ml/lib/python3.8/site-packages/torchvision/transforms/transforms.py:287: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(epoch: 1, iters: 100, time: 0.089, data: 0.090) G_GAN: 0.753 G_L1: 66.626 D_real: 0.269 D_fake: 0.329 \n",
      "(epoch: 1, iters: 200, time: 0.093, data: 0.002) G_GAN: 0.517 G_L1: 36.313 D_real: 0.213 D_fake: 0.164 \n",
      "(epoch: 1, iters: 300, time: 0.103, data: 0.001) G_GAN: 1.349 G_L1: 47.777 D_real: 0.091 D_fake: 0.110 \n",
      "(epoch: 1, iters: 400, time: 0.280, data: 0.001) G_GAN: 0.958 G_L1: 47.398 D_real: 0.067 D_fake: 0.113 \n",
      "End of epoch 1 / 200 \t Time Taken: 24 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agysar/miniconda3/envs/advanced_ml/lib/python3.8/site-packages/torchvision/transforms/transforms.py:287: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(opt_pix2pix.epoch_count, opt_pix2pix.n_epochs + opt_pix2pix.n_epochs_decay + 1):    # outer loop for different epochs; we save the model by <epoch_count>, <epoch_count>+<save_latest_freq>\n",
    "    epoch_start_time = time.time()  # timer for entire epoch\n",
    "    iter_data_time = time.time()    # timer for data loading per iteration\n",
    "    epoch_iter = 0                  # the number of training iterations in current epoch, reset to 0 every epoch\n",
    "    visualizer.reset()              # reset the visualizer: make sure it saves the results to HTML at least once every epoch\n",
    "    model.update_learning_rate()    # update learning rates in the beginning of every epoch.\n",
    "    for i, data in enumerate(dataset):  # inner loop within one epoch\n",
    "        iter_start_time = time.time()  # timer for computation per iteration\n",
    "        if total_iters % opt_pix2pix.print_freq == 0:\n",
    "            t_data = iter_start_time - iter_data_time\n",
    "\n",
    "        total_iters += opt_pix2pix.batch_size\n",
    "        epoch_iter += opt_pix2pix.batch_size\n",
    "        model.set_input(data)         # unpack data from dataset and apply preprocessing\n",
    "        model.optimize_parameters()   # calculate loss functions, get gradients, update network weights\n",
    "\n",
    "        if total_iters % opt.display_freq == 0:   # display images on visdom and save images to a HTML file\n",
    "            save_result = total_iters % opt_pix2pix.update_html_freq == 0\n",
    "            model.compute_visuals()\n",
    "            visualizer.display_current_results(model.get_current_visuals(), epoch, save_result)\n",
    "\n",
    "        if total_iters % opt.print_freq == 0:    # print training losses and save logging information to the disk\n",
    "            losses = model.get_current_losses()\n",
    "            t_comp = (time.time() - iter_start_time) / opt_pix2pix.batch_size\n",
    "            visualizer.print_current_losses(epoch, epoch_iter, losses, t_comp, t_data)\n",
    "            if opt_pix2pix.display_id > 0:\n",
    "                visualizer.plot_current_losses(epoch, float(epoch_iter) / dataset_size, losses)\n",
    "\n",
    "        if total_iters % opt_pix2pix.save_latest_freq == 0:   # cache our latest model every <save_latest_freq> iterations\n",
    "            print('saving the latest model (epoch %d, total_iters %d)' % (epoch, total_iters))\n",
    "            save_suffix = 'iter_%d' % total_iters if opt_pix2pix.save_by_iter else 'latest'\n",
    "            model.save_networks(save_suffix)\n",
    "\n",
    "        iter_data_time = time.time()\n",
    "    if epoch % opt_pix2pix.save_epoch_freq == 0:              # cache our model every <save_epoch_freq> epochs\n",
    "        print('saving the model at the end of epoch %d, iters %d' % (epoch, total_iters))\n",
    "        model.save_networks('latest')\n",
    "        model.save_networks(epoch)\n",
    "\n",
    "    print('End of epoch %d / %d \\t Time Taken: %d sec' % (epoch, opt_pix2pix.n_epochs + opt_pix2pix.n_epochs_decay, time.time() - epoch_start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./checkpoints/pix2pix: LSGAN/newplot.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Можно заметить, что pix2pix довольно уверенно обучился за 200 эпох. В среднем обучение на одной эпохе заняло 25 секунд."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пример сгенерированных изображений из train_data:\n",
    "| epoch #  | real image | real stylized image | fake stylized image |\n",
    "| ------------- | ------------- | ------------- | ------------- |\n",
    "| __1__  | <img src=\"./checkpoints/pix2pix: LSGAN/web/images/epoch001_real_A.png\"> | <img src=\"./checkpoints/pix2pix: LSGAN/web/images/epoch001_real_B.png\"> | <img src=\"./checkpoints/pix2pix: LSGAN/web/images/epoch001_fake_B.png\"> |\n",
    "| __50__  | <img src=\"./checkpoints/pix2pix: LSGAN/web/images/epoch050_real_A.png\"> | <img src=\"./checkpoints/pix2pix: LSGAN/web/images/epoch050_real_B.png\"> | <img src=\"./checkpoints/pix2pix: LSGAN/web/images/epoch050_fake_B.png\"> |\n",
    "| __100__  | <img src=\"./checkpoints/pix2pix: LSGAN/web/images/epoch100_real_A.png\"> | <img src=\"./checkpoints/pix2pix: LSGAN/web/images/epoch100_real_B.png\"> | <img src=\"./checkpoints/pix2pix: LSGAN/web/images/epoch100_fake_B.png\"> |\n",
    "| __150__  | <img src=\"./checkpoints/pix2pix: LSGAN/web/images/epoch150_real_A.png\"> | <img src=\"./checkpoints/pix2pix: LSGAN/web/images/epoch150_real_B.png\"> | <img src=\"./checkpoints/pix2pix: LSGAN/web/images/epoch150_fake_B.png\"> |\n",
    "| __200__  | <img src=\"./checkpoints/pix2pix: LSGAN/web/images/epoch200_real_A.png\"> | <img src=\"./checkpoints/pix2pix: LSGAN/web/images/epoch200_real_B.png\"> | <img src=\"./checkpoints/pix2pix: LSGAN/web/images/epoch200_fake_B.png\"> |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Options ---------------\n",
      "             aspect_ratio: 1.0                           \n",
      "               batch_size: 1                             \n",
      "          checkpoints_dir: ./checkpoints                 \n",
      "                crop_size: 256                           \n",
      "                 dataroot: ./facades/                    \n",
      "             dataset_mode: single                        \n",
      "                direction: AtoB                          \n",
      "          display_winsize: 256                           \n",
      "                    epoch: latest                        \n",
      "                     eval: False                         \n",
      "                  gpu_ids: 0                             \n",
      "                init_gain: 0.02                          \n",
      "                init_type: normal                        \n",
      "                 input_nc: 3                             \n",
      "                  isTrain: False                         \t[default: None]\n",
      "                load_iter: 0                             \t[default: 0]\n",
      "                load_size: 256                           \n",
      "         max_dataset_size: inf                           \n",
      "                    model: test                          \n",
      "             model_suffix:                               \n",
      "               n_layers_D: 3                             \n",
      "                     name: experiment_name               \n",
      "                      ndf: 64                            \n",
      "                     netD: basic                         \n",
      "                     netG: resnet_9blocks                \n",
      "                      ngf: 64                            \n",
      "               no_dropout: False                         \n",
      "                  no_flip: False                         \n",
      "                     norm: instance                      \n",
      "                 num_test: 50                            \n",
      "              num_threads: 4                             \n",
      "                output_nc: 3                             \n",
      "                    phase: test                          \n",
      "               preprocess: resize_and_crop               \n",
      "              results_dir: ./results/                    \n",
      "           serial_batches: False                         \n",
      "                   suffix:                               \n",
      "                use_wandb: False                         \n",
      "                  verbose: False                         \n",
      "----------------- End -------------------\n",
      "dataset [AlignedDataset] was created\n",
      "initialize network with normal\n",
      "model [Pix2PixModel] was created\n",
      "loading the model from ./checkpoints/pix2pix: LSGAN/latest_net_G.pth\n",
      "---------- Networks initialized -------------\n",
      "[Network G] Total number of parameters : 11.378 M\n",
      "-----------------------------------------------\n",
      "creating web directory ./results/pix2pix: LSGAN/test_latest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agysar/miniconda3/envs/advanced_ml/lib/python3.8/site-packages/torchvision/transforms/transforms.py:287: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing (0000)-th image... ['./facades/test/1.jpg']\n",
      "processing (0005)-th image... ['./facades/test/103.jpg']\n",
      "processing (0010)-th image... ['./facades/test/12.jpg']\n",
      "processing (0015)-th image... ['./facades/test/17.jpg']\n",
      "processing (0020)-th image... ['./facades/test/21.jpg']\n",
      "processing (0025)-th image... ['./facades/test/26.jpg']\n",
      "processing (0030)-th image... ['./facades/test/30.jpg']\n",
      "processing (0035)-th image... ['./facades/test/35.jpg']\n",
      "processing (0040)-th image... ['./facades/test/4.jpg']\n",
      "processing (0045)-th image... ['./facades/test/44.jpg']\n"
     ]
    }
   ],
   "source": [
    "opt_pix2pix_test = TestOptions().parse()  # get test options\n",
    "opt_pix2pix_test.model = \"pix2pix\"\n",
    "opt_pix2pix_test.dataset_mode = \"aligned\"\n",
    "opt_pix2pix_test.name = \"pix2pix: LSGAN\"\n",
    "opt_pix2pix_test.lambda_L1 = 100.0\n",
    "# hard-code some parameters for test`\n",
    "opt_pix2pix_test.num_threads = 0   # test code only supports num_threads = 0\n",
    "opt_pix2pix_test.batch_size = 1    # test code only supports batch_size = 1\n",
    "opt_pix2pix_test.serial_batches = True  # disable data shuffling; comment this line if results on randomly chosen images are needed.\n",
    "opt_pix2pix_test.no_flip = True    # no flip; comment this line if results on flipped images are needed.\n",
    "opt_pix2pix_test.display_id = -1   # no visdom display; the test code saves the results to a HTML file.\n",
    "dataset = create_dataset(opt_pix2pix_test)  # create a dataset given opt.dataset_mode and other options\n",
    "model = create_model(opt_pix2pix_test)      # create a model given opt.model and other options\n",
    "model.setup(opt_pix2pix_test)               # regular setup: load and print networks; create schedulers\n",
    "\n",
    "# initialize logger\n",
    "if opt_pix2pix_test.use_wandb:\n",
    "    wandb_run = wandb.init(project='CycleGAN-and-pix2pix', name=opt_pix2pix_test.name, config=opt_pix2pix_test) if not wandb.run else wandb.run\n",
    "    wandb_run._label(repo='CycleGAN-and-pix2pix')\n",
    "\n",
    "# create a website\n",
    "web_dir = os.path.join(opt_pix2pix_test.results_dir, opt_pix2pix_test.name, '{}_{}'.format(opt_pix2pix_test.phase, opt_pix2pix_test.epoch))  # define the website directory\n",
    "if opt_pix2pix_test.load_iter > 0:  # load_iter is 0 by default\n",
    "    web_dir = '{:s}_iter{:d}'.format(web_dir, opt_pix2pix_test.load_iter)\n",
    "print('creating web directory', web_dir)\n",
    "webpage = html.HTML(web_dir, 'Experiment = %s, Phase = %s, Epoch = %s' % (opt_pix2pix_test.name, opt_pix2pix_test.phase, opt_pix2pix_test.epoch))\n",
    "# test with eval mode. This only affects layers like batchnorm and dropout.\n",
    "# For [pix2pix]: we use batchnorm and dropout in the original pix2pix. You can experiment it with and without eval() mode.\n",
    "# For [CycleGAN]: It should not affect CycleGAN as CycleGAN uses instancenorm without dropout.\n",
    "if opt_pix2pix_test.eval:\n",
    "    model.eval()\n",
    "for i, data in enumerate(dataset):\n",
    "    if i >= opt_pix2pix_test.num_test:  # only apply our model to opt.num_test images.\n",
    "        break\n",
    "    model.set_input(data)  # unpack data from data loader\n",
    "    model.test()           # run inference\n",
    "    visuals = model.get_current_visuals()  # get image results\n",
    "    img_path = model.get_image_paths()     # get image paths\n",
    "    if i % 5 == 0:  # save images to an HTML file\n",
    "        print('processing (%04d)-th image... %s' % (i, img_path))\n",
    "    save_images(webpage, visuals, img_path, aspect_ratio=opt_pix2pix_test.aspect_ratio, width=opt_pix2pix_test.display_winsize, use_wandb=opt_pix2pix_test.use_wandb)\n",
    "webpage.save()  # save the HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Но при этом на тесте, результаты удручающие: генерируется шум."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пример сгенерированных изображений из test_data:\n",
    "|  #  | real image | real stylized image | fake stylized image |\n",
    "| ------------- | ------------- | ------------- | ------------- |\n",
    "| __1__  | <img src=\"./results/pix2pix: LSGAN/test_latest/images/1_real_A.png\"> | <img src=\"./results/pix2pix: LSGAN/test_latest/images/1_real_B.png\"> | <img src=\"./results/pix2pix: LSGAN/test_latest/images/1_fake_B.png\"> |\n",
    "| __2__  | <img src=\"./results/pix2pix: LSGAN/test_latest/images/2_real_A.png\"> | <img src=\"./results/pix2pix: LSGAN/test_latest/images/2_real_B.png\"> | <img src=\"./results/pix2pix: LSGAN/test_latest/images/2_fake_B.png\"> |\n",
    "| __3__  | <img src=\"./results/pix2pix: LSGAN/test_latest/images/3_real_A.png\"> | <img src=\"./results/pix2pix: LSGAN/test_latest/images/3_real_B.png\"> | <img src=\"./results/pix2pix: LSGAN/test_latest/images/3_fake_B.png\"> |\n",
    "| __4__  | <img src=\"./results/pix2pix: LSGAN/test_latest/images/4_real_A.png\"> | <img src=\"./results/pix2pix: LSGAN/test_latest/images/4_real_B.png\"> | <img src=\"./results/pix2pix: LSGAN/test_latest/images/4_fake_B.png\"> |\n",
    "| __10__  | <img src=\"./results/pix2pix: LSGAN/test_latest/images/10_real_A.png\"> | <img src=\"./results/pix2pix: LSGAN/test_latest/images/10_real_B.png\"> | <img src=\"./results/pix2pix: LSGAN/test_latest/images/10_fake_B.png\"> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CycleGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Options ---------------\n",
      "               batch_size: 1                             \n",
      "                    beta1: 0.5                           \n",
      "          checkpoints_dir: ./checkpoints                 \n",
      "           continue_train: False                         \n",
      "                crop_size: 256                           \n",
      "                 dataroot: ./facades/                    \n",
      "             dataset_mode: unaligned                     \n",
      "                direction: AtoB                          \n",
      "              display_env: main                          \n",
      "             display_freq: 400                           \n",
      "               display_id: 1                             \n",
      "            display_ncols: 4                             \n",
      "             display_port: 8097                          \n",
      "           display_server: http://localhost              \n",
      "          display_winsize: 256                           \n",
      "                    epoch: latest                        \n",
      "              epoch_count: 1                             \n",
      "                 gan_mode: lsgan                         \n",
      "                  gpu_ids: 0                             \n",
      "                init_gain: 0.02                          \n",
      "                init_type: normal                        \n",
      "                 input_nc: 3                             \n",
      "                  isTrain: True                          \t[default: None]\n",
      "                 lambda_A: 10.0                          \n",
      "                 lambda_B: 10.0                          \n",
      "          lambda_identity: 0.5                           \n",
      "                load_iter: 0                             \t[default: 0]\n",
      "                load_size: 286                           \n",
      "                       lr: 0.0002                        \n",
      "           lr_decay_iters: 50                            \n",
      "                lr_policy: linear                        \n",
      "         max_dataset_size: inf                           \n",
      "                    model: cycle_gan                     \n",
      "                 n_epochs: 100                           \n",
      "           n_epochs_decay: 100                           \n",
      "               n_layers_D: 3                             \n",
      "                     name: experiment_name               \n",
      "                      ndf: 64                            \n",
      "                     netD: basic                         \n",
      "                     netG: resnet_9blocks                \n",
      "                      ngf: 64                            \n",
      "               no_dropout: True                          \n",
      "                  no_flip: False                         \n",
      "                  no_html: False                         \n",
      "                     norm: instance                      \n",
      "              num_threads: 4                             \n",
      "                output_nc: 3                             \n",
      "                    phase: train                         \n",
      "                pool_size: 50                            \n",
      "               preprocess: resize_and_crop               \n",
      "               print_freq: 100                           \n",
      "             save_by_iter: False                         \n",
      "          save_epoch_freq: 5                             \n",
      "         save_latest_freq: 5000                          \n",
      "           serial_batches: False                         \n",
      "                   suffix:                               \n",
      "         update_html_freq: 1000                          \n",
      "                use_wandb: False                         \n",
      "                  verbose: False                         \n",
      "----------------- End -------------------\n"
     ]
    }
   ],
   "source": [
    "opt_cyclegan = TrainOptions().parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_cyclegan.model = \"cycle_gan\"\n",
    "opt_cyclegan.dataset_mode = \"unaligned\"\n",
    "opt_cyclegan.name = \"Cyclegan: LSGAN\"\n",
    "opt_cyclegan.lambda_L1 = 100.0\n",
    "opt_cyclegan.n_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset [AlignedDataset] was created\n",
      "The number of training images = 400\n"
     ]
    }
   ],
   "source": [
    "dataset = create_dataset(opt_cyclegan)\n",
    "dataset_size = len(dataset)    # get the number of images in the dataset.\n",
    "print('The number of training images = %d' % dataset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize network with normal\n",
      "initialize network with normal\n",
      "initialize network with normal\n",
      "initialize network with normal\n",
      "model [CycleGANModel] was created\n",
      "---------- Networks initialized -------------\n",
      "[Network G_A] Total number of parameters : 11.378 M\n",
      "[Network G_B] Total number of parameters : 11.378 M\n",
      "[Network D_A] Total number of parameters : 2.765 M\n",
      "[Network D_B] Total number of parameters : 2.765 M\n",
      "-----------------------------------------------\n",
      "create web directory ./checkpoints/Cyclegan: LSGAN/web...\n"
     ]
    }
   ],
   "source": [
    "model = create_model(opt_cyclegan)      # create a model given opt.model and other options\n",
    "model.setup(opt_cyclegan)               # regular setup: load and print networks; create schedulers\n",
    "visualizer = Visualizer(opt_cyclegan)   # create a visualizer that display/save images and plots\n",
    "total_iters = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate 0.0002000 -> 0.0002000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agysar/miniconda3/envs/advanced_ml/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/home/agysar/miniconda3/envs/advanced_ml/lib/python3.8/site-packages/torchvision/transforms/transforms.py:287: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(opt_cyclegan.epoch_count, opt_cyclegan.n_epochs + opt_cyclegan.n_epochs_decay + 1):    # outer loop for different epochs; we save the model by <epoch_count>, <epoch_count>+<save_latest_freq>\n",
    "    epoch_start_time = time.time()  # timer for entire epoch\n",
    "    iter_data_time = time.time()    # timer for data loading per iteration\n",
    "    epoch_iter = 0                  # the number of training iterations in current epoch, reset to 0 every epoch\n",
    "    visualizer.reset()              # reset the visualizer: make sure it saves the results to HTML at least once every epoch\n",
    "    model.update_learning_rate()    # update learning rates in the beginning of every epoch.\n",
    "    for i, data in enumerate(dataset):  # inner loop within one epoch\n",
    "        iter_start_time = time.time()  # timer for computation per iteration\n",
    "        if total_iters % opt_cyclegan.print_freq == 0:\n",
    "            t_data = iter_start_time - iter_data_time\n",
    "\n",
    "        total_iters += opt_cyclegan.batch_size\n",
    "        epoch_iter += opt_cyclegan.batch_size\n",
    "        model.set_input(data)         # unpack data from dataset and apply preprocessing\n",
    "        model.optimize_parameters()   # calculate loss functions, get gradients, update network weights\n",
    "\n",
    "        if total_iters % opt_cyclegan.display_freq == 0:   # display images on visdom and save images to a HTML file\n",
    "            save_result = total_iters % opt_cyclegan.update_html_freq == 0\n",
    "            model.compute_visuals()\n",
    "            visualizer.display_current_results(model.get_current_visuals(), epoch, save_result)\n",
    "\n",
    "        if total_iters % opt_cyclegan.print_freq == 0:    # print training losses and save logging information to the disk\n",
    "            losses = model.get_current_losses()\n",
    "            t_comp = (time.time() - iter_start_time) / opt_cyclegan.batch_size\n",
    "            visualizer.print_current_losses(epoch, epoch_iter, losses, t_comp, t_data)\n",
    "            if opt_cyclegan.display_id > 0:\n",
    "                visualizer.plot_current_losses(epoch, float(epoch_iter) / dataset_size, losses)\n",
    "\n",
    "        if total_iters % opt_cyclegan.save_latest_freq == 0:   # cache our latest model every <save_latest_freq> iterations\n",
    "            print('saving the latest model (epoch %d, total_iters %d)' % (epoch, total_iters))\n",
    "            save_suffix = 'iter_%d' % total_iters if opt_cyclegan.save_by_iter else 'latest'\n",
    "            model.save_networks(save_suffix)\n",
    "\n",
    "        iter_data_time = time.time()\n",
    "    if epoch % opt_cyclegan.save_epoch_freq == 0:              # cache our model every <save_epoch_freq> epochs\n",
    "        print('saving the model at the end of epoch %d, iters %d' % (epoch, total_iters))\n",
    "        model.save_networks('latest')\n",
    "        model.save_networks(epoch)\n",
    "\n",
    "    print('End of epoch %d / %d \\t Time Taken: %d sec' % (epoch, opt_cyclegan.n_epochs + opt_cyclegan.n_epochs_decay, time.time() - epoch_start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./checkpoints/Cyclegan: LSGAN/newplot (1).png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### В свою очередь CycleGAN обучается почти в 5 раз медленнее чем pix2pix. В среднем обучение на одной эпохе заняло 120 секунд. \n",
    "#### При этом качество генерируемых изображений сопоставимо с pix2pix уже на 25 эпохе."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пример сгенерированных изображений:\n",
    "| epoch #  | real image | real stylized image | fake stylized image |\n",
    "| ------------- | ------------- | ------------- | ------------- |\n",
    "| __1__  | <img src=\"./checkpoints/Cyclegan: LSGAN/web/images/epoch001_real_A.png\"> | <img src=\"./checkpoints/Cyclegan: LSGAN/web/images/epoch001_real_B.png\"> | <img src=\"./checkpoints/Cyclegan: LSGAN/web/images/epoch001_rec_B.png\"> |\n",
    "| __5__  | <img src=\"./checkpoints/Cyclegan: LSGAN/web/images/epoch005_real_A.png\"> | <img src=\"./checkpoints/Cyclegan: LSGAN/web/images/epoch005_real_B.png\"> | <img src=\"./checkpoints/Cyclegan: LSGAN/web/images/epoch005_rec_B.png\"> |\n",
    "| __10__  | <img src=\"./checkpoints/Cyclegan: LSGAN/web/images/epoch010_real_A.png\"> | <img src=\"./checkpoints/Cyclegan: LSGAN/web/images/epoch010_real_B.png\"> | <img src=\"./checkpoints/Cyclegan: LSGAN/web/images/epoch010_rec_B.png\"> |\n",
    "| __15__  | <img src=\"./checkpoints/Cyclegan: LSGAN/web/images/epoch015_real_A.png\"> | <img src=\"./checkpoints/Cyclegan: LSGAN/web/images/epoch015_real_B.png\"> | <img src=\"./checkpoints/Cyclegan: LSGAN/web/images/epoch015_rec_B.png\"> |\n",
    "| __25__  | <img src=\"./checkpoints/Cyclegan: LSGAN/web/images/epoch025_real_A.png\"> | <img src=\"./checkpoints/Cyclegan: LSGAN/web/images/epoch025_real_B.png\"> | <img src=\"./checkpoints/Cyclegan: LSGAN/web/images/epoch025_rec_B.png\"> |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Options ---------------\n",
      "             aspect_ratio: 1.0                           \n",
      "               batch_size: 1                             \n",
      "          checkpoints_dir: ./checkpoints                 \n",
      "                crop_size: 256                           \n",
      "                 dataroot: ./facades/                    \n",
      "             dataset_mode: single                        \n",
      "                direction: AtoB                          \n",
      "          display_winsize: 256                           \n",
      "                    epoch: latest                        \n",
      "                     eval: False                         \n",
      "                  gpu_ids: 0                             \n",
      "                init_gain: 0.02                          \n",
      "                init_type: normal                        \n",
      "                 input_nc: 3                             \n",
      "                  isTrain: False                         \t[default: None]\n",
      "                load_iter: 0                             \t[default: 0]\n",
      "                load_size: 256                           \n",
      "         max_dataset_size: inf                           \n",
      "                    model: test                          \n",
      "             model_suffix:                               \n",
      "               n_layers_D: 3                             \n",
      "                     name: experiment_name               \n",
      "                      ndf: 64                            \n",
      "                     netD: basic                         \n",
      "                     netG: resnet_9blocks                \n",
      "                      ngf: 64                            \n",
      "               no_dropout: False                         \n",
      "                  no_flip: False                         \n",
      "                     norm: instance                      \n",
      "                 num_test: 50                            \n",
      "              num_threads: 4                             \n",
      "                output_nc: 3                             \n",
      "                    phase: test                          \n",
      "               preprocess: resize_and_crop               \n",
      "              results_dir: ./results/                    \n",
      "           serial_batches: False                         \n",
      "                   suffix:                               \n",
      "                use_wandb: False                         \n",
      "                  verbose: False                         \n",
      "----------------- End -------------------\n",
      "dataset [AlignedDataset] was created\n",
      "initialize network with normal\n",
      "initialize network with normal\n",
      "model [CycleGANModel] was created\n",
      "loading the model from ./checkpoints/Cyclegan: LSGAN/latest_net_G_A.pth\n",
      "loading the model from ./checkpoints/Cyclegan: LSGAN/latest_net_G_B.pth\n",
      "---------- Networks initialized -------------\n",
      "[Network G_A] Total number of parameters : 11.378 M\n",
      "[Network G_B] Total number of parameters : 11.378 M\n",
      "-----------------------------------------------\n",
      "creating web directory ./results/Cyclegan: LSGAN/test_latest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agysar/miniconda3/envs/advanced_ml/lib/python3.8/site-packages/torchvision/transforms/transforms.py:287: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing (0000)-th image... ['./facades/test/1.jpg']\n",
      "processing (0005)-th image... ['./facades/test/103.jpg']\n",
      "processing (0010)-th image... ['./facades/test/12.jpg']\n",
      "processing (0015)-th image... ['./facades/test/17.jpg']\n",
      "processing (0020)-th image... ['./facades/test/21.jpg']\n",
      "processing (0025)-th image... ['./facades/test/26.jpg']\n",
      "processing (0030)-th image... ['./facades/test/30.jpg']\n",
      "processing (0035)-th image... ['./facades/test/35.jpg']\n",
      "processing (0040)-th image... ['./facades/test/4.jpg']\n",
      "processing (0045)-th image... ['./facades/test/44.jpg']\n"
     ]
    }
   ],
   "source": [
    "opt_cyclegan_test = TestOptions().parse()  # get test options\n",
    "opt_cyclegan_test.model = \"cycle_gan\"\n",
    "opt_cyclegan_test.dataset_mode = \"aligned\"\n",
    "opt_cyclegan_test.name = \"Cyclegan: LSGAN\"\n",
    "opt_cyclegan_test.lambda_L1 = 100.0\n",
    "# hard-code some parameters for test`\n",
    "opt_cyclegan_test.num_threads = 0   # test code only supports num_threads = 0\n",
    "opt_cyclegan_test.batch_size = 1    # test code only supports batch_size = 1\n",
    "opt_cyclegan_test.serial_batches = True  # disable data shuffling; comment this line if results on randomly chosen images are needed.\n",
    "opt_cyclegan_test.no_flip = True    # no flip; comment this line if results on flipped images are needed.\n",
    "opt_cyclegan_test.display_id = -1   # no visdom display; the test code saves the results to a HTML file.\n",
    "dataset = create_dataset(opt_cyclegan_test)  # create a dataset given opt.dataset_mode and other options\n",
    "model = create_model(opt_cyclegan_test)      # create a model given opt.model and other options\n",
    "model.setup(opt_cyclegan_test)               # regular setup: load and print networks; create schedulers\n",
    "\n",
    "# initialize logger\n",
    "if opt_cyclegan_test.use_wandb:\n",
    "    wandb_run = wandb.init(project='CycleGAN-and-pix2pix', name=opt_cyclegan_test.name, config=opt_cyclegan_test) if not wandb.run else wandb.run\n",
    "    wandb_run._label(repo='CycleGAN-and-pix2pix')\n",
    "\n",
    "# create a website\n",
    "web_dir = os.path.join(opt_cyclegan_test.results_dir, opt_cyclegan_test.name, '{}_{}'.format(opt_cyclegan_test.phase, opt_cyclegan_test.epoch))  # define the website directory\n",
    "if opt_cyclegan_test.load_iter > 0:  # load_iter is 0 by default\n",
    "    web_dir = '{:s}_iter{:d}'.format(web_dir, opt_cyclegan_test.load_iter)\n",
    "print('creating web directory', web_dir)\n",
    "webpage = html.HTML(web_dir, 'Experiment = %s, Phase = %s, Epoch = %s' % (opt_cyclegan_test.name, opt_cyclegan_test.phase, opt_cyclegan_test.epoch))\n",
    "# test with eval mode. This only affects layers like batchnorm and dropout.\n",
    "# For [cyclegan]: we use batchnorm and dropout in the original cyclegan. You can experiment it with and without eval() mode.\n",
    "# For [CycleGAN]: It should not affect CycleGAN as CycleGAN uses instancenorm without dropout.\n",
    "if opt_cyclegan_test.eval:\n",
    "    model.eval()\n",
    "for i, data in enumerate(dataset):\n",
    "    if i >= opt_cyclegan_test.num_test:  # only apply our model to opt.num_test images.\n",
    "        break\n",
    "    model.set_input(data)  # unpack data from data loader\n",
    "    model.test()           # run inference\n",
    "    visuals = model.get_current_visuals()  # get image results\n",
    "    img_path = model.get_image_paths()     # get image paths\n",
    "    if i % 5 == 0:  # save images to an HTML file\n",
    "        print('processing (%04d)-th image... %s' % (i, img_path))\n",
    "    save_images(webpage, visuals, img_path, aspect_ratio=opt_cyclegan_test.aspect_ratio, width=opt_cyclegan_test.display_winsize, use_wandb=opt_cyclegan_test.use_wandb)\n",
    "webpage.save()  # save the HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### В отличие от pix2pix, CycleGAN может улавливать контуры стиля уже после 25+ эпох."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пример сгенерированных изображений из test_data:\n",
    "|  #  | real image | real stylized image | fake stylized image |\n",
    "| ------------- | ------------- | ------------- | ------------- |\n",
    "| __1__  | <img src=\"./results/Cyclegan: LSGAN/test_latest/images/1_real_A.png\"> | <img src=\"./results/Cyclegan: LSGAN/test_latest/images/1_real_B.png\"> | <img src=\"./results/Cyclegan: LSGAN/test_latest/images/1_fake_B.png\"> |\n",
    "| __2__  | <img src=\"./results/Cyclegan: LSGAN/test_latest/images/2_real_A.png\"> | <img src=\"./results/Cyclegan: LSGAN/test_latest/images/2_real_B.png\"> | <img src=\"./results/Cyclegan: LSGAN/test_latest/images/2_fake_B.png\"> |\n",
    "| __3__  | <img src=\"./results/Cyclegan: LSGAN/test_latest/images/3_real_A.png\"> | <img src=\"./results/Cyclegan: LSGAN/test_latest/images/3_real_B.png\"> | <img src=\"./results/Cyclegan: LSGAN/test_latest/images/3_fake_B.png\"> |\n",
    "| __4__  | <img src=\"./results/Cyclegan: LSGAN/test_latest/images/4_real_A.png\"> | <img src=\"./results/Cyclegan: LSGAN/test_latest/images/4_real_B.png\"> | <img src=\"./results/Cyclegan: LSGAN/test_latest/images/4_fake_B.png\"> |\n",
    "| __10__  | <img src=\"./results/Cyclegan: LSGAN/test_latest/images/10_real_A.png\"> | <img src=\"./results/Cyclegan: LSGAN/test_latest/images/10_real_B.png\"> | <img src=\"./results/Cyclegan: LSGAN/test_latest/images/10_fake_B.png\"> |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
